{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import BertModel\n",
    "# from tokenization_kobert import KoBertTokenizer\n",
    "# MODEL_NAME = \"monologg/kobert\"\n",
    "# model = BertModel.from_pretrainead(MODEL_NAME)\n",
    "# kobert_tokenizer = KoBertTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraModel, ElectraTokenizer, AutoTokenizer\n",
    "\n",
    "model = ElectraModel.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "# tokenizer = ElectraTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KoElectra tokenizer:\n",
      "['천둥', '##번', '##개', '##를', '동반', '##한', '소나기', '##가', '34', '##번', '내렸', '##습', '##니다', '.', '상당히', '많이', '내렸', '##는데'] \n",
      " ['장갑', '##을', '낀', '사람', '##이', '밭', '##에', '있', '##다', '그', '사람', '##이', '허리', '##를', '숙이', '##고', '있', '##다', '그', '사람', '##이', '작업', '##을', '한다']\n"
     ]
    }
   ],
   "source": [
    "# \"Kobert\"와 \"KoElectra\" 간 tokenizing 결과는 유사하다\n",
    "# QnA 성능이 더 좋은 \"KoElectra\"로 실험\n",
    "\n",
    "# print('KoBert tokenizer:')\n",
    "# print(\n",
    "# kobert_tokenizer.tokenize(\"천둥번개를 동반한 소나기가 34번 내렸습니다. 상당히 많이 내렸는데\"),'\\n',\n",
    "# kobert_tokenizer.tokenize(\"장갑을 낀 사람이 밭에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다\"))\n",
    "# print('\\n')\n",
    "print('KoElectra tokenizer:')\n",
    "print(\n",
    "tokenizer.tokenize(\"천둥번개를 동반한 소나기가 34번 내렸습니다. 상당히 많이 내렸는데\"),'\\n',\n",
    "tokenizer.tokenize(\"장갑을 낀 사람이 밭에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pororo로 postagging 후 명사에 masking 추가 + rnn보다 강건한 transformer 계열 모델 사용\n",
    "\n",
    "# from pororo import Pororo\n",
    "# pos = Pororo(task=\"pos\", lang=\"ko\")\n",
    "# sent = \"검은색 옷을 입은 사람이 들판에 있다\\n그 사람이 설명을 한다\\n그 사람이 두 손을 펼친다\"\n",
    "# pos(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 34000, 34999)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# unused vocab 확인\n",
    "unused_tok2idx = {k:v for k,v in tokenizer.get_vocab().items() if 'unused' in k}\n",
    "len(unused_tok2idx), min(unused_tok2idx.values()), max(unused_tok2idx.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "우박 ['우', '##박']\n",
      "설원 ['설', '##원']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "<KoElectra.Tokenizer, Mecab 을 이용한 vocab matrix 생성>\n",
    "\n",
    "matrix = np.zeros((vocab, 768))\n",
    "\n",
    "- 목적 : 정답단어 분절X, unk 어휘는 일단 무시\n",
    "\n",
    "1. 정답 단어 중 분절 단어 모으기\n",
    "\n",
    "add_tokens = [new1, new2...]\n",
    "num_added_tokens = Tokenizer.add_tokens(add_tokens) # 34000 ~\n",
    "\n",
    "vocab = Tokenizer.vocab\n",
    "i2w = {idx:k for w, idx in vocab}\n",
    "\n",
    "vector1 = get_word_emb\n",
    "vector2 = get_new_emb\n",
    "\n",
    "answers embedding:\n",
    "- \n",
    "\"\"\"\n",
    "answers = [\n",
    "      \"눈이라고 말함\",\n",
    "      \"우박이라고 말함\",\n",
    "      \"바람이라고 말함\",\n",
    "      \"소나기라고 말함\",\n",
    "      \"구름이라고 말함\"\n",
    "      \"설원이라고 말함\"\n",
    "    ]\n",
    "\n",
    "new_tokens = []\n",
    "\n",
    "for answer in answers:\n",
    "\n",
    "    for morph in mecab(answer):\n",
    "        \n",
    "        answer_tokened = tokenizer.tokenize(morph)\n",
    "\n",
    "        if len(answer_tokened)==1 or morph in new_tokens:\n",
    "            continue\n",
    "\n",
    "        # subword 분절 여부 check -> 해당 단어 저장\n",
    "        print(morph, answer_tokened)\n",
    "        new_tokens.append(morph)\n",
    "\n",
    "new_tokens\n",
    "added_num = tokenizer.add_tokens(new_tokens)\n",
    "added_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "embsize = model.config.embedding_size\n",
    "init_vocab_cnt = len(tokenizer)\n",
    "final_vocab_cnt = init_vocab_cnt +len(new_tokens)\n",
    "\n",
    "matrix = np.zeros((final_vocab_cnt, embsize))\n",
    "w2idx = sorted(tokenizer.vocab.items(), key=lambda x: x[1])\n",
    "w2idx += [(w, idx) for idx, w in enumerate(new_tokens, init_vocab_cnt)]\n",
    "\n",
    "vocab_words = [w for w, _ in w2idx]\n",
    "\n",
    "batch_size = 3000\n",
    "for i in tqdm(range(0, init_vocab_cnt, batch_size)):\n",
    "    word_list = vocab_words[i:min(i+batch_size, init_vocab_cnt)]\n",
    "    wordvecs = get_word_vector_batch(word_list, tokenizer, model, 4)\n",
    "    if i==0:\n",
    "        print(wordvecs.size())\n",
    "    matrix[i:min(i+batch_size, init_vocab_cnt), :] = wordvecs.detach().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = ['우박', '설원']\n",
    "pool_method = 4\n",
    "encoded_batch = tokenizer.batch_encode_plus(sents, return_tensors=\"pt\",padding=True)\n",
    "output = get_hidden_states(encoded_batch, model)\n",
    "output = output[pool_method-1]\n",
    "\n",
    "token_ids_word_li = []\n",
    "for i in range(len(sents)):\n",
    "    idx = get_word_idx(sents[i], words[i])\n",
    "    token_ids_word = np.where(np.array(encoded_batch.word_ids(i)) == idx)\n",
    "    token_ids_word_li.append(token_ids_word)\n",
    "# Only select the tokens that constitute the requested word\n",
    "word_tokens_outputs = []\n",
    "for i in range(len(token_ids_word_li)):\n",
    "    word_tokens_output = output[i, token_ids_word_li[i][0],:].mean(dim=0).squeeze()\n",
    "    word_tokens_outputs.append(word_tokens_output)\n",
    "word_tokens_outputs = torch.stack(word_tokens_outputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write into ./data/video-narr/tokenizer_vocab_35002_768.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "vocab_save_fname = f'./data/video-narr/tokenizer_vocab_{final_vocab_cnt}_{embsize}.json'\n",
    "print(\"Write into %s\" % vocab_save_fname)\n",
    "\n",
    "with open(vocab_save_fname, 'w') as file:\n",
    "    json.dump(tokenizer.vocab, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write into ./tokenizer_vocab_35004_768.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./vocab.txt',)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "vocab_save_fname = f'./tokenizer_vocab_{final_vocab_cnt}_{embsize}.json'\n",
    "print(\"Write into %s\" % vocab_save_fname)\n",
    "tokenizer.save_vocabulary('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('[PAD]', 0),\n",
       " ('[UNK]', 1),\n",
       " ('[CLS]', 2),\n",
       " ('[SEP]', 3),\n",
       " ('[MASK]', 4),\n",
       " ('!', 5),\n",
       " ('\"', 6),\n",
       " ('#', 7),\n",
       " ('$', 8),\n",
       " ('%', 9),\n",
       " ('&', 10),\n",
       " (\"'\", 11),\n",
       " ('(', 12),\n",
       " (')', 13),\n",
       " ('*', 14),\n",
       " ('+', 15),\n",
       " (',', 16),\n",
       " ('-', 17),\n",
       " ('.', 18),\n",
       " ('/', 19),\n",
       " ('0', 20),\n",
       " ('1', 21),\n",
       " ('2', 22),\n",
       " ('3', 23),\n",
       " ('4', 24),\n",
       " ('5', 25),\n",
       " ('6', 26),\n",
       " ('7', 27),\n",
       " ('8', 28),\n",
       " ('9', 29),\n",
       " (':', 30),\n",
       " (';', 31),\n",
       " ('<', 32),\n",
       " ('=', 33),\n",
       " ('>', 34),\n",
       " ('?', 35),\n",
       " ('@', 36),\n",
       " ('A', 37),\n",
       " ('B', 38),\n",
       " ('C', 39),\n",
       " ('D', 40),\n",
       " ('E', 41),\n",
       " ('F', 42),\n",
       " ('G', 43),\n",
       " ('H', 44),\n",
       " ('I', 45),\n",
       " ('J', 46),\n",
       " ('K', 47),\n",
       " ('L', 48),\n",
       " ('M', 49),\n",
       " ('N', 50),\n",
       " ('O', 51),\n",
       " ('P', 52),\n",
       " ('Q', 53),\n",
       " ('R', 54),\n",
       " ('S', 55),\n",
       " ('T', 56),\n",
       " ('U', 57),\n",
       " ('V', 58),\n",
       " ('W', 59),\n",
       " ('X', 60),\n",
       " ('Y', 61),\n",
       " ('Z', 62),\n",
       " ('[', 63),\n",
       " ('\\\\', 64),\n",
       " (']', 65),\n",
       " ('^', 66),\n",
       " ('_', 67),\n",
       " ('`', 68),\n",
       " ('a', 69),\n",
       " ('b', 70),\n",
       " ('c', 71),\n",
       " ('d', 72),\n",
       " ('e', 73),\n",
       " ('f', 74),\n",
       " ('g', 75),\n",
       " ('h', 76),\n",
       " ('i', 77),\n",
       " ('j', 78),\n",
       " ('k', 79),\n",
       " ('l', 80),\n",
       " ('m', 81),\n",
       " ('n', 82),\n",
       " ('o', 83),\n",
       " ('p', 84),\n",
       " ('q', 85),\n",
       " ('r', 86),\n",
       " ('s', 87),\n",
       " ('t', 88),\n",
       " ('u', 89),\n",
       " ('v', 90),\n",
       " ('w', 91),\n",
       " ('x', 92),\n",
       " ('y', 93),\n",
       " ('z', 94),\n",
       " ('{', 95),\n",
       " ('|', 96),\n",
       " ('}', 97),\n",
       " ('~', 98),\n",
       " ('§', 99),\n",
       " ('©', 100),\n",
       " ('®', 101),\n",
       " ('°', 102),\n",
       " ('±', 103),\n",
       " ('²', 104),\n",
       " ('³', 105),\n",
       " ('´', 106),\n",
       " ('·', 107),\n",
       " ('¹', 108),\n",
       " ('×', 109),\n",
       " ('ä', 110),\n",
       " ('é', 111),\n",
       " ('ö', 112),\n",
       " ('÷', 113),\n",
       " ('̈', 114),\n",
       " ('̣', 115),\n",
       " ('̥', 116),\n",
       " ('α', 117),\n",
       " ('β', 118),\n",
       " ('μ', 119),\n",
       " ('а', 120),\n",
       " ('б', 121),\n",
       " ('в', 122),\n",
       " ('г', 123),\n",
       " ('е', 124),\n",
       " ('з', 125),\n",
       " ('и', 126),\n",
       " ('к', 127),\n",
       " ('л', 128),\n",
       " ('н', 129),\n",
       " ('о', 130),\n",
       " ('п', 131),\n",
       " ('р', 132),\n",
       " ('с', 133),\n",
       " ('т', 134),\n",
       " ('ы', 135),\n",
       " ('ь', 136),\n",
       " ('я', 137),\n",
       " ('๑', 138),\n",
       " ('ᄒ', 139),\n",
       " ('ᆞ', 140),\n",
       " ('–', 141),\n",
       " ('—', 142),\n",
       " ('―', 143),\n",
       " ('‘', 144),\n",
       " ('’', 145),\n",
       " ('“', 146),\n",
       " ('”', 147),\n",
       " ('•', 148),\n",
       " ('․', 149),\n",
       " ('‥', 150),\n",
       " ('…', 151),\n",
       " ('‧', 152),\n",
       " ('′', 153),\n",
       " ('″', 154),\n",
       " ('※', 155),\n",
       " ('‼', 156),\n",
       " ('₁', 157),\n",
       " ('₂', 158),\n",
       " ('₩', 159),\n",
       " ('℃', 160),\n",
       " ('ℓ', 161),\n",
       " ('™', 162),\n",
       " ('Ⅰ', 163),\n",
       " ('Ⅱ', 164),\n",
       " ('Ⅲ', 165),\n",
       " ('Ⅳ', 166),\n",
       " ('Ⅴ', 167),\n",
       " ('↑', 168),\n",
       " ('→', 169),\n",
       " ('↓', 170),\n",
       " ('↔', 171),\n",
       " ('↘', 172),\n",
       " ('↙', 173),\n",
       " ('⇒', 174),\n",
       " ('∙', 175),\n",
       " ('∼', 176),\n",
       " ('≪', 177),\n",
       " ('≫', 178),\n",
       " ('⊙', 179),\n",
       " ('⋅', 180),\n",
       " ('①', 181),\n",
       " ('②', 182),\n",
       " ('③', 183),\n",
       " ('④', 184),\n",
       " ('⑤', 185),\n",
       " ('⑥', 186),\n",
       " ('⑦', 187),\n",
       " ('⑧', 188),\n",
       " ('⑨', 189),\n",
       " ('⑴', 190),\n",
       " ('⑵', 191),\n",
       " ('⑶', 192),\n",
       " ('⑷', 193),\n",
       " ('ⓐ', 194),\n",
       " ('ⓑ', 195),\n",
       " ('ⓒ', 196),\n",
       " ('─', 197),\n",
       " ('━', 198),\n",
       " ('│', 199),\n",
       " ('┃', 200),\n",
       " ('└', 201),\n",
       " ('├', 202),\n",
       " ('■', 203),\n",
       " ('□', 204),\n",
       " ('▣', 205),\n",
       " ('▪', 206),\n",
       " ('▫', 207),\n",
       " ('▲', 208),\n",
       " ('△', 209),\n",
       " ('▶', 210),\n",
       " ('▷', 211),\n",
       " ('▼', 212),\n",
       " ('▽', 213),\n",
       " ('◀', 214),\n",
       " ('◆', 215),\n",
       " ('◇', 216),\n",
       " ('◈', 217),\n",
       " ('○', 218),\n",
       " ('◎', 219),\n",
       " ('●', 220),\n",
       " ('★', 221),\n",
       " ('☆', 222),\n",
       " ('☎', 223),\n",
       " ('☏', 224),\n",
       " ('☕', 225),\n",
       " ('☞', 226),\n",
       " ('☺', 227),\n",
       " ('♀', 228),\n",
       " ('♂', 229),\n",
       " ('♠', 230),\n",
       " ('♡', 231),\n",
       " ('♣', 232),\n",
       " ('♥', 233),\n",
       " ('✅', 234),\n",
       " ('✌', 235),\n",
       " ('✔', 236),\n",
       " ('✨', 237),\n",
       " ('❗', 238),\n",
       " ('❣', 239),\n",
       " ('❤', 240),\n",
       " ('➡', 241),\n",
       " ('⠀', 242),\n",
       " ('⭐', 243),\n",
       " ('。', 244),\n",
       " ('〃', 245),\n",
       " ('〈', 246),\n",
       " ('〉', 247),\n",
       " ('《', 248),\n",
       " ('》', 249),\n",
       " ('「', 250),\n",
       " ('」', 251),\n",
       " ('『', 252),\n",
       " ('』', 253),\n",
       " ('【', 254),\n",
       " ('】', 255),\n",
       " ('〓', 256),\n",
       " ('〔', 257),\n",
       " ('〕', 258),\n",
       " ('〰', 259),\n",
       " ('い', 260),\n",
       " ('か', 261),\n",
       " ('て', 262),\n",
       " ('に', 263),\n",
       " ('の', 264),\n",
       " ('り', 265),\n",
       " ('る', 266),\n",
       " ('ん', 267),\n",
       " ('ニ', 268),\n",
       " ('ン', 269),\n",
       " ('・', 270),\n",
       " ('ー', 271),\n",
       " ('ㄱ', 272),\n",
       " ('ㄲ', 273),\n",
       " ('ㄴ', 274),\n",
       " ('ㄷ', 275),\n",
       " ('ㄸ', 276),\n",
       " ('ㄹ', 277),\n",
       " ('ㅁ', 278),\n",
       " ('ㅂ', 279),\n",
       " ('ㅃ', 280),\n",
       " ('ㅅ', 281),\n",
       " ('ㅆ', 282),\n",
       " ('ㅇ', 283),\n",
       " ('ㅈ', 284),\n",
       " ('ㅉ', 285),\n",
       " ('ㅊ', 286),\n",
       " ('ㅋ', 287),\n",
       " ('ㅌ', 288),\n",
       " ('ㅍ', 289),\n",
       " ('ㅎ', 290),\n",
       " ('ㅏ', 291),\n",
       " ('ㅐ', 292),\n",
       " ('ㅑ', 293),\n",
       " ('ㅓ', 294),\n",
       " ('ㅔ', 295),\n",
       " ('ㅕ', 296),\n",
       " ('ㅗ', 297),\n",
       " ('ㅛ', 298),\n",
       " ('ㅜ', 299),\n",
       " ('ㅠ', 300),\n",
       " ('ㅡ', 301),\n",
       " ('ㅣ', 302),\n",
       " ('ㅤ', 303),\n",
       " ('ㆍ', 304),\n",
       " ('㈎', 305),\n",
       " ('㈏', 306),\n",
       " ('㈐', 307),\n",
       " ('㈑', 308),\n",
       " ('㈔', 309),\n",
       " ('㈜', 310),\n",
       " ('㉠', 311),\n",
       " ('㉡', 312),\n",
       " ('㉢', 313),\n",
       " ('㉣', 314),\n",
       " ('㉮', 315),\n",
       " ('㉯', 316),\n",
       " ('㉰', 317),\n",
       " ('㎉', 318),\n",
       " ('㎍', 319),\n",
       " ('㎎', 320),\n",
       " ('㎏', 321),\n",
       " ('㎒', 322),\n",
       " ('㎓', 323),\n",
       " ('㎖', 324),\n",
       " ('㎘', 325),\n",
       " ('㎚', 326),\n",
       " ('㎛', 327),\n",
       " ('㎜', 328),\n",
       " ('㎝', 329),\n",
       " ('㎞', 330),\n",
       " ('㎡', 331),\n",
       " ('㎢', 332),\n",
       " ('㎥', 333),\n",
       " ('㎸', 334),\n",
       " ('㎾', 335),\n",
       " ('㎿', 336),\n",
       " ('㏃', 337),\n",
       " ('㏄', 338),\n",
       " ('㏈', 339),\n",
       " ('㏊', 340),\n",
       " ('一', 341),\n",
       " ('丁', 342),\n",
       " ('七', 343),\n",
       " ('丈', 344),\n",
       " ('三', 345),\n",
       " ('上', 346),\n",
       " ('下', 347),\n",
       " ('不', 348),\n",
       " ('丑', 349),\n",
       " ('且', 350),\n",
       " ('世', 351),\n",
       " ('丘', 352),\n",
       " ('丙', 353),\n",
       " ('丞', 354),\n",
       " ('中', 355),\n",
       " ('丸', 356),\n",
       " ('丹', 357),\n",
       " ('主', 358),\n",
       " ('乃', 359),\n",
       " ('久', 360),\n",
       " ('之', 361),\n",
       " ('乎', 362),\n",
       " ('乘', 363),\n",
       " ('乙', 364),\n",
       " ('九', 365),\n",
       " ('也', 366),\n",
       " ('乡', 367),\n",
       " ('乳', 368),\n",
       " ('乾', 369),\n",
       " ('亂', 370),\n",
       " ('了', 371),\n",
       " ('事', 372),\n",
       " ('二', 373),\n",
       " ('于', 374),\n",
       " ('云', 375),\n",
       " ('互', 376),\n",
       " ('五', 377),\n",
       " ('井', 378),\n",
       " ('亞', 379),\n",
       " ('亡', 380),\n",
       " ('交', 381),\n",
       " ('亥', 382),\n",
       " ('亦', 383),\n",
       " ('亨', 384),\n",
       " ('享', 385),\n",
       " ('京', 386),\n",
       " ('亭', 387),\n",
       " ('亮', 388),\n",
       " ('人', 389),\n",
       " ('仁', 390),\n",
       " ('今', 391),\n",
       " ('介', 392),\n",
       " ('仕', 393),\n",
       " ('他', 394),\n",
       " ('付', 395),\n",
       " ('仙', 396),\n",
       " ('代', 397),\n",
       " ('令', 398),\n",
       " ('以', 399),\n",
       " ('仰', 400),\n",
       " ('仲', 401),\n",
       " ('件', 402),\n",
       " ('任', 403),\n",
       " ('企', 404),\n",
       " ('伊', 405),\n",
       " ('伏', 406),\n",
       " ('伐', 407),\n",
       " ('休', 408),\n",
       " ('会', 409),\n",
       " ('伯', 410),\n",
       " ('伸', 411),\n",
       " ('似', 412),\n",
       " ('伽', 413),\n",
       " ('但', 414),\n",
       " ('位', 415),\n",
       " ('低', 416),\n",
       " ('住', 417),\n",
       " ('佐', 418),\n",
       " ('何', 419),\n",
       " ('余', 420),\n",
       " ('佛', 421),\n",
       " ('作', 422),\n",
       " ('佳', 423),\n",
       " ('使', 424),\n",
       " ('來', 425),\n",
       " ('例', 426),\n",
       " ('侍', 427),\n",
       " ('供', 428),\n",
       " ('依', 429),\n",
       " ('侯', 430),\n",
       " ('侵', 431),\n",
       " ('便', 432),\n",
       " ('係', 433),\n",
       " ('俊', 434),\n",
       " ('俗', 435),\n",
       " ('保', 436),\n",
       " ('信', 437),\n",
       " ('修', 438),\n",
       " ('俱', 439),\n",
       " ('倂', 440),\n",
       " ('倉', 441),\n",
       " ('個', 442),\n",
       " ('倍', 443),\n",
       " ('倒', 444),\n",
       " ('候', 445),\n",
       " ('借', 446),\n",
       " ('値', 447),\n",
       " ('倫', 448),\n",
       " ('倭', 449),\n",
       " ('假', 450),\n",
       " ('偉', 451),\n",
       " ('偏', 452),\n",
       " ('停', 453),\n",
       " ('健', 454),\n",
       " ('側', 455),\n",
       " ('偶', 456),\n",
       " ('傅', 457),\n",
       " ('備', 458),\n",
       " ('傳', 459),\n",
       " ('債', 460),\n",
       " ('傷', 461),\n",
       " ('傾', 462),\n",
       " ('像', 463),\n",
       " ('僕', 464),\n",
       " ('僞', 465),\n",
       " ('僧', 466),\n",
       " ('價', 467),\n",
       " ('儀', 468),\n",
       " ('億', 469),\n",
       " ('儒', 470),\n",
       " ('償', 471),\n",
       " ('優', 472),\n",
       " ('允', 473),\n",
       " ('元', 474),\n",
       " ('兄', 475),\n",
       " ('充', 476),\n",
       " ('兆', 477),\n",
       " ('先', 478),\n",
       " ('光', 479),\n",
       " ('克', 480),\n",
       " ('免', 481),\n",
       " ('兒', 482),\n",
       " ('入', 483),\n",
       " ('內', 484),\n",
       " ('全', 485),\n",
       " ('兩', 486),\n",
       " ('兪', 487),\n",
       " ('八', 488),\n",
       " ('公', 489),\n",
       " ('六', 490),\n",
       " ('兮', 491),\n",
       " ('共', 492),\n",
       " ('兵', 493),\n",
       " ('其', 494),\n",
       " ('具', 495),\n",
       " ('典', 496),\n",
       " ('兼', 497),\n",
       " ('冊', 498),\n",
       " ('再', 499),\n",
       " ('冠', 500),\n",
       " ('冥', 501),\n",
       " ('冬', 502),\n",
       " ('凉', 503),\n",
       " ('凡', 504),\n",
       " ('凶', 505),\n",
       " ('出', 506),\n",
       " ('刀', 507),\n",
       " ('分', 508),\n",
       " ('切', 509),\n",
       " ('刊', 510),\n",
       " ('刑', 511),\n",
       " ('列', 512),\n",
       " ('初', 513),\n",
       " ('判', 514),\n",
       " ('別', 515),\n",
       " ('利', 516),\n",
       " ('到', 517),\n",
       " ('制', 518),\n",
       " ('券', 519),\n",
       " ('刺', 520),\n",
       " ('刻', 521),\n",
       " ('則', 522),\n",
       " ('前', 523),\n",
       " ('剛', 524),\n",
       " ('副', 525),\n",
       " ('割', 526),\n",
       " ('創', 527),\n",
       " ('劃', 528),\n",
       " ('劇', 529),\n",
       " ('劉', 530),\n",
       " ('劍', 531),\n",
       " ('力', 532),\n",
       " ('功', 533),\n",
       " ('加', 534),\n",
       " ('助', 535),\n",
       " ('勇', 536),\n",
       " ('勒', 537),\n",
       " ('動', 538),\n",
       " ('務', 539),\n",
       " ('勝', 540),\n",
       " ('勞', 541),\n",
       " ('勢', 542),\n",
       " ('勤', 543),\n",
       " ('勳', 544),\n",
       " ('勸', 545),\n",
       " ('勿', 546),\n",
       " ('包', 547),\n",
       " ('化', 548),\n",
       " ('北', 549),\n",
       " ('匠', 550),\n",
       " ('區', 551),\n",
       " ('十', 552),\n",
       " ('千', 553),\n",
       " ('升', 554),\n",
       " ('午', 555),\n",
       " ('半', 556),\n",
       " ('卑', 557),\n",
       " ('卒', 558),\n",
       " ('卓', 559),\n",
       " ('協', 560),\n",
       " ('南', 561),\n",
       " ('博', 562),\n",
       " ('卜', 563),\n",
       " ('占', 564),\n",
       " ('卦', 565),\n",
       " ('卯', 566),\n",
       " ('印', 567),\n",
       " ('危', 568),\n",
       " ('却', 569),\n",
       " ('卷', 570),\n",
       " ('卽', 571),\n",
       " ('卿', 572),\n",
       " ('厚', 573),\n",
       " ('原', 574),\n",
       " ('去', 575),\n",
       " ('參', 576),\n",
       " ('又', 577),\n",
       " ('及', 578),\n",
       " ('友', 579),\n",
       " ('反', 580),\n",
       " ('叔', 581),\n",
       " ('取', 582),\n",
       " ('受', 583),\n",
       " ('叢', 584),\n",
       " ('口', 585),\n",
       " ('古', 586),\n",
       " ('句', 587),\n",
       " ('只', 588),\n",
       " ('召', 589),\n",
       " ('可', 590),\n",
       " ('台', 591),\n",
       " ('史', 592),\n",
       " ('右', 593),\n",
       " ('司', 594),\n",
       " ('各', 595),\n",
       " ('合', 596),\n",
       " ('吉', 597),\n",
       " ('同', 598),\n",
       " ('名', 599),\n",
       " ('后', 600),\n",
       " ('吏', 601),\n",
       " ('向', 602),\n",
       " ('君', 603),\n",
       " ('否', 604),\n",
       " ('含', 605),\n",
       " ('吳', 606),\n",
       " ('吾', 607),\n",
       " ('呂', 608),\n",
       " ('告', 609),\n",
       " ('周', 610),\n",
       " ('呪', 611),\n",
       " ('味', 612),\n",
       " ('呼', 613),\n",
       " ('命', 614),\n",
       " ('和', 615),\n",
       " ('咸', 616),\n",
       " ('哀', 617),\n",
       " ('品', 618),\n",
       " ('哈', 619),\n",
       " ('哉', 620),\n",
       " ('員', 621),\n",
       " ('哭', 622),\n",
       " ('哲', 623),\n",
       " ('唐', 624),\n",
       " ('唯', 625),\n",
       " ('唱', 626),\n",
       " ('商', 627),\n",
       " ('問', 628),\n",
       " ('啓', 629),\n",
       " ('善', 630),\n",
       " ('喜', 631),\n",
       " ('喪', 632),\n",
       " ('單', 633),\n",
       " ('嘉', 634),\n",
       " ('嘗', 635),\n",
       " ('器', 636),\n",
       " ('嚴', 637),\n",
       " ('四', 638),\n",
       " ('回', 639),\n",
       " ('因', 640),\n",
       " ('固', 641),\n",
       " ('国', 642),\n",
       " ('圈', 643),\n",
       " ('國', 644),\n",
       " ('圍', 645),\n",
       " ('園', 646),\n",
       " ('圓', 647),\n",
       " ('圖', 648),\n",
       " ('團', 649),\n",
       " ('土', 650),\n",
       " ('在', 651),\n",
       " ('圭', 652),\n",
       " ('地', 653),\n",
       " ('址', 654),\n",
       " ('坂', 655),\n",
       " ('均', 656),\n",
       " ('坊', 657),\n",
       " ('坐', 658),\n",
       " ('坡', 659),\n",
       " ('坤', 660),\n",
       " ('坪', 661),\n",
       " ('垂', 662),\n",
       " ('型', 663),\n",
       " ('城', 664),\n",
       " ('域', 665),\n",
       " ('執', 666),\n",
       " ('培', 667),\n",
       " ('基', 668),\n",
       " ('堂', 669),\n",
       " ('堅', 670),\n",
       " ('堯', 671),\n",
       " ('報', 672),\n",
       " ('場', 673),\n",
       " ('塔', 674),\n",
       " ('塚', 675),\n",
       " ('塵', 676),\n",
       " ('境', 677),\n",
       " ('墓', 678),\n",
       " ('增', 679),\n",
       " ('墨', 680),\n",
       " ('墳', 681),\n",
       " ('壁', 682),\n",
       " ('壇', 683),\n",
       " ('壓', 684),\n",
       " ('壤', 685),\n",
       " ('士', 686),\n",
       " ('壬', 687),\n",
       " ('壯', 688),\n",
       " ('壽', 689),\n",
       " ('夏', 690),\n",
       " ('夕', 691),\n",
       " ('外', 692),\n",
       " ('多', 693),\n",
       " ('夜', 694),\n",
       " ('夢', 695),\n",
       " ('大', 696),\n",
       " ('天', 697),\n",
       " ('太', 698),\n",
       " ('夫', 699),\n",
       " ('央', 700),\n",
       " ('失', 701),\n",
       " ('夷', 702),\n",
       " ('奇', 703),\n",
       " ('奈', 704),\n",
       " ('奉', 705),\n",
       " ('奎', 706),\n",
       " ('奏', 707),\n",
       " ('契', 708),\n",
       " ('奪', 709),\n",
       " ('女', 710),\n",
       " ('奴', 711),\n",
       " ('好', 712),\n",
       " ('如', 713),\n",
       " ('妃', 714),\n",
       " ('妄', 715),\n",
       " ('妓', 716),\n",
       " ('妙', 717),\n",
       " ('妻', 718),\n",
       " ('始', 719),\n",
       " ('姑', 720),\n",
       " ('姓', 721),\n",
       " ('委', 722),\n",
       " ('姜', 723),\n",
       " ('姬', 724),\n",
       " ('威', 725),\n",
       " ('婆', 726),\n",
       " ('婚', 727),\n",
       " ('婢', 728),\n",
       " ('婦', 729),\n",
       " ('子', 730),\n",
       " ('孔', 731),\n",
       " ('字', 732),\n",
       " ('存', 733),\n",
       " ('孝', 734),\n",
       " ('孟', 735),\n",
       " ('季', 736),\n",
       " ('孤', 737),\n",
       " ('学', 738),\n",
       " ('孫', 739),\n",
       " ('學', 740),\n",
       " ('宇', 741),\n",
       " ('守', 742),\n",
       " ('安', 743),\n",
       " ('宋', 744),\n",
       " ('完', 745),\n",
       " ('宏', 746),\n",
       " ('宗', 747),\n",
       " ('官', 748),\n",
       " ('宙', 749),\n",
       " ('定', 750),\n",
       " ('宜', 751),\n",
       " ('客', 752),\n",
       " ('宣', 753),\n",
       " ('室', 754),\n",
       " ('宮', 755),\n",
       " ('宰', 756),\n",
       " ('害', 757),\n",
       " ('宴', 758),\n",
       " ('家', 759),\n",
       " ('容', 760),\n",
       " ('宿', 761),\n",
       " ('寂', 762),\n",
       " ('寄', 763),\n",
       " ('寅', 764),\n",
       " ('密', 765),\n",
       " ('富', 766),\n",
       " ('寒', 767),\n",
       " ('察', 768),\n",
       " ('寡', 769),\n",
       " ('實', 770),\n",
       " ('寧', 771),\n",
       " ('審', 772),\n",
       " ('寫', 773),\n",
       " ('寬', 774),\n",
       " ('寶', 775),\n",
       " ('寸', 776),\n",
       " ('寺', 777),\n",
       " ('封', 778),\n",
       " ('射', 779),\n",
       " ('將', 780),\n",
       " ('專', 781),\n",
       " ('尉', 782),\n",
       " ('尊', 783),\n",
       " ('尋', 784),\n",
       " ('對', 785),\n",
       " ('導', 786),\n",
       " ('小', 787),\n",
       " ('少', 788),\n",
       " ('尖', 789),\n",
       " ('尙', 790),\n",
       " ('就', 791),\n",
       " ('尹', 792),\n",
       " ('尺', 793),\n",
       " ('尼', 794),\n",
       " ('尾', 795),\n",
       " ('局', 796),\n",
       " ('居', 797),\n",
       " ('屈', 798),\n",
       " ('屋', 799),\n",
       " ('展', 800),\n",
       " ('層', 801),\n",
       " ('履', 802),\n",
       " ('屬', 803),\n",
       " ('屯', 804),\n",
       " ('山', 805),\n",
       " ('岐', 806),\n",
       " ('岡', 807),\n",
       " ('岩', 808),\n",
       " ('岳', 809),\n",
       " ('岸', 810),\n",
       " ('峯', 811),\n",
       " ('峰', 812),\n",
       " ('峴', 813),\n",
       " ('島', 814),\n",
       " ('崇', 815),\n",
       " ('崎', 816),\n",
       " ('崔', 817),\n",
       " ('嶺', 818),\n",
       " ('巖', 819),\n",
       " ('川', 820),\n",
       " ('州', 821),\n",
       " ('巡', 822),\n",
       " ('工', 823),\n",
       " ('左', 824),\n",
       " ('巨', 825),\n",
       " ('巫', 826),\n",
       " ('差', 827),\n",
       " ('己', 828),\n",
       " ('已', 829),\n",
       " ('巳', 830),\n",
       " ('巴', 831),\n",
       " ('市', 832),\n",
       " ('布', 833),\n",
       " ('希', 834),\n",
       " ('帖', 835),\n",
       " ('帝', 836),\n",
       " ('帥', 837),\n",
       " ('師', 838),\n",
       " ('席', 839),\n",
       " ('帳', 840),\n",
       " ('帶', 841),\n",
       " ('常', 842),\n",
       " ('幕', 843),\n",
       " ('干', 844),\n",
       " ('平', 845),\n",
       " ('年', 846),\n",
       " ('幸', 847),\n",
       " ('幹', 848),\n",
       " ('幻', 849),\n",
       " ('幼', 850),\n",
       " ('幽', 851),\n",
       " ('幾', 852),\n",
       " ('庄', 853),\n",
       " ('床', 854),\n",
       " ('序', 855),\n",
       " ('底', 856),\n",
       " ('店', 857),\n",
       " ('庚', 858),\n",
       " ('府', 859),\n",
       " ('度', 860),\n",
       " ('座', 861),\n",
       " ('庫', 862),\n",
       " ('庭', 863),\n",
       " ('庵', 864),\n",
       " ('庶', 865),\n",
       " ('康', 866),\n",
       " ('庸', 867),\n",
       " ('廉', 868),\n",
       " ('廟', 869),\n",
       " ('廢', 870),\n",
       " ('廣', 871),\n",
       " ('廳', 872),\n",
       " ('延', 873),\n",
       " ('廷', 874),\n",
       " ('建', 875),\n",
       " ('弊', 876),\n",
       " ('式', 877),\n",
       " ('弓', 878),\n",
       " ('弔', 879),\n",
       " ('引', 880),\n",
       " ('弗', 881),\n",
       " ('弘', 882),\n",
       " ('弟', 883),\n",
       " ('弱', 884),\n",
       " ('張', 885),\n",
       " ('强', 886),\n",
       " ('弼', 887),\n",
       " ('彈', 888),\n",
       " ('彌', 889),\n",
       " ('形', 890),\n",
       " ('彦', 891),\n",
       " ('彩', 892),\n",
       " ('彭', 893),\n",
       " ('影', 894),\n",
       " ('役', 895),\n",
       " ('彼', 896),\n",
       " ('往', 897),\n",
       " ('征', 898),\n",
       " ('待', 899),\n",
       " ('律', 900),\n",
       " ('後', 901),\n",
       " ('徐', 902),\n",
       " ('徒', 903),\n",
       " ('得', 904),\n",
       " ('從', 905),\n",
       " ('御', 906),\n",
       " ('復', 907),\n",
       " ('微', 908),\n",
       " ('徵', 909),\n",
       " ('德', 910),\n",
       " ('徹', 911),\n",
       " ('徽', 912),\n",
       " ('心', 913),\n",
       " ('必', 914),\n",
       " ('忌', 915),\n",
       " ('忍', 916),\n",
       " ('志', 917),\n",
       " ('忘', 918),\n",
       " ('忠', 919),\n",
       " ('快', 920),\n",
       " ('念', 921),\n",
       " ('忽', 922),\n",
       " ('怒', 923),\n",
       " ('思', 924),\n",
       " ('急', 925),\n",
       " ('性', 926),\n",
       " ('怨', 927),\n",
       " ('怪', 928),\n",
       " ('恐', 929),\n",
       " ('恒', 930),\n",
       " ('恥', 931),\n",
       " ('恨', 932),\n",
       " ('恩', 933),\n",
       " ('恭', 934),\n",
       " ('息', 935),\n",
       " ('悅', 936),\n",
       " ('悟', 937),\n",
       " ('患', 938),\n",
       " ('悲', 939),\n",
       " ('情', 940),\n",
       " ('惑', 941),\n",
       " ('惟', 942),\n",
       " ('惠', 943),\n",
       " ('惡', 944),\n",
       " ('惱', 945),\n",
       " ('想', 946),\n",
       " ('愁', 947),\n",
       " ('意', 948),\n",
       " ('愚', 949),\n",
       " ('愛', 950),\n",
       " ('感', 951),\n",
       " ('愼', 952),\n",
       " ('慈', 953),\n",
       " ('態', 954),\n",
       " ('慕', 955),\n",
       " ('慣', 956),\n",
       " ('慧', 957),\n",
       " ('慮', 958),\n",
       " ('慶', 959),\n",
       " ('慾', 960),\n",
       " ('憂', 961),\n",
       " ('憲', 962),\n",
       " ('應', 963),\n",
       " ('懷', 964),\n",
       " ('戊', 965),\n",
       " ('戌', 966),\n",
       " ('成', 967),\n",
       " ('我', 968),\n",
       " ('戒', 969),\n",
       " ('或', 970),\n",
       " ('戰', 971),\n",
       " ('戱', 972),\n",
       " ('戴', 973),\n",
       " ('戶', 974),\n",
       " ('房', 975),\n",
       " ('所', 976),\n",
       " ('手', 977),\n",
       " ('才', 978),\n",
       " ('打', 979),\n",
       " ('扶', 980),\n",
       " ('批', 981),\n",
       " ('承', 982),\n",
       " ('技', 983),\n",
       " ('投', 984),\n",
       " ('抗', 985),\n",
       " ('折', 986),\n",
       " ('抵', 987),\n",
       " ('拒', 988),\n",
       " ('拘', 989),\n",
       " ('招', 990),\n",
       " ('拜', 991),\n",
       " ('括', 992),\n",
       " ('持', 993),\n",
       " ('指', 994),\n",
       " ('振', 995),\n",
       " ('捕', 996),\n",
       " ('授', 997),\n",
       " ('掌', 998),\n",
       " ('排', 999),\n",
       " ...]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(k,v) for k,v in tokenizer.vocab.items()], key = lambda x : x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('./data/video-narr/tokenizer/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35002 0\n"
     ]
    }
   ],
   "source": [
    "cnt = len(tokenizer)\n",
    "print(cnt, added_num)\n",
    "# tokenizer.save(f'./data/video-narr/tokenizer_vocab_{cnt}_{embsize}.json', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['우', '##박이', '##라고', '말', '##함'], ['우박', '이', '라고', '말', '함'])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenize 개수가 같아도 subword 불일치\n",
    "tokenizer.tokenize('우박이라고 말함'), mecab('우박이라고 말함')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"소나기 vs 구름, 작년\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['자전거', '를', '들이받', '음'], ['들이', '##받'])"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mecab('자전거를 들이받음'), tokenizer.tokenize('들이받')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]', '안녕', '내', '사랑', '[SEP]']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize('안녕 내 사랑', add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./data/video-narr/tokenizer/tokenizer_config.json',\n",
       " './data/video-narr/tokenizer/special_tokens_map.json',\n",
       " './data/video-narr/tokenizer/vocab.txt',\n",
       " './data/video-narr/tokenizer/added_tokens.json',\n",
       " './data/video-narr/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_vocab_cnt = len(tokenizer)\n",
    "embsize = model.config.embedding_size\n",
    "vocab_save_fname = f'./data/video-narr/tokenizer_vocab_{final_vocab_cnt}_{embsize}'\n",
    "save_dir = './data/video-narr/tokenizer/'\n",
    "tokenizer.save_pretrained(save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35002"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_loaded = AutoTokenizer.from_pretrained('./data/video-narr/tokenizer/', use_fast=True)\n",
    "len(tokenizer_loaded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from konlpy.tag import Mecab\n",
    "import numpy as np\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def todevice(device, tokenized):\n",
    "    return {k:v.to(device) for k,v in tokenized.items()}\n",
    "\n",
    "mecab = Mecab().morphs\n",
    "a = '장갑을 낀 사람이 밭에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다'\n",
    "# Mecab().pos(\"보호대\")\n",
    "\n",
    "def get_hidden_states(encoded, model):\n",
    "    \"\"\"Push input IDs through model. Stack and sum `layers` (last four by default).\n",
    "    Select only those subword token outputs that belong to our word of interest\n",
    "    and average them.\"\"\"\n",
    "    \n",
    "    # with torch.no_grad():\n",
    "    #     output = model(**encoded, output_hidden_states = True)\n",
    "    \n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "    \n",
    "    last_hidden_state, hidden_states, attentions = model(**todevice(device, encoded),\n",
    "                                                                    output_hidden_states = True, \n",
    "                                                                    output_attentions=True,\n",
    "                                                                    return_dict=False) # return_dict=True 시 outputs = model(**pt)로 return 받아야\n",
    "\n",
    "    # # 512 words\n",
    "    # print('last hidden state shape:', last_hidden_state.shape)\n",
    "\n",
    "    # # 13:(initial embeddings + 12 BERT layers)\n",
    "    # print('hidden state shape:', torch.stack(hidden_states).size()) # (layer, N, T, D)\n",
    "\n",
    "    # print('attentions shape:', torch.stack(attentions).size())\n",
    "\n",
    "    # Remove batch-dim, Swap dimensions 0 and 1.\n",
    "    token_embeddings = torch.stack(hidden_states).permute(1,2,0,3) # (N, T, layer, D)\n",
    "\n",
    "    #https://is-rajapaksha.medium.com/bert-word-embeddings-deep-dive-32f6214f02bf\n",
    "\n",
    "    # initial embeddings can be taken from 0th layer of hidden states\n",
    "    # hidden_states = (maxlen, layers, dimension)\n",
    "    word_embed_2 = token_embeddings[:,:,0,:] # [N, T, L, D] -> [N, T, D]\n",
    "    # sum of all hidden states\n",
    "    word_embed_3 = token_embeddings.sum(2) # [N, T, L, D] -> [N, T, D]\n",
    "    # sum of second to last layer\n",
    "    word_embed_4 = token_embeddings[:,:,2:,:].sum(2) # [N, T, 13, D] -> [N, T, 11, D] -> [N, T, D]\n",
    "    # sum of last four layer\n",
    "    word_embed_5 = token_embeddings[:,:,-4:,:].sum(2) # [N, T, 4, D] -> [N, T, D]\n",
    "    # concatenate last four layers\n",
    "    word_embed_6 = torch.cat([token_embeddings[:,:,i,:] for i in range(-4, -1+1)], dim=1) # [N, T, 13, D] -> [N, T, 4, D] -> [N, T, 4*D]\n",
    "    return word_embed_2, word_embed_3, word_embed_4, word_embed_5, word_embed_6\n",
    "\n",
    "    # # Get all hidden states\n",
    "    # states = output.hidden_states # 13, 1, 512, 768\n",
    "\n",
    "    # # Stack and sum all requested layers\n",
    "    # output = torch.stack([states[i] for i in layers]).sum(0).squeeze()\n",
    "\n",
    "    # return output\n",
    "\n",
    "def get_word_idx(sent: str, word: str):\n",
    "    return mecab(sent).index(word)\n",
    "\n",
    "def get_orgword_vector(sent, word, vocab_idx, tokenizer, model, pool_method:int):\n",
    "    # sent : tokenized and joined with space; ex. \"나 는 학교 에 간 다\" or \"학교\"\n",
    "    # sent = ' '.join(mecab('우박이라고함'))\n",
    "    \n",
    "    encoded = tokenizer.encode_plus(sent, return_tensors=\"pt\")\n",
    "\n",
    "    if vocab_idx > len(tokenizer):\n",
    "        idx = get_word_idx(sent, word)\n",
    "        token_ids_word = np.where(np.array(encoded.word_ids()) == idx)\n",
    "    else:\n",
    "         token_ids_word = (np.array([1]),) # [CLS] token\n",
    "\n",
    "    output = get_hidden_states(encoded, model)\n",
    "    output = output[pool_method-1]\n",
    "\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[token_ids_word[0]].squeeze()\n",
    "    return word_tokens_output if len(token_ids_word[0])==1 else word_tokens_output.mean(dim=0)\n",
    "\n",
    "def get_orgword_vector_batch(sents, words, tokenizer, model, pool_method:int):\n",
    "    # sent : tokenized and joined with space; ex. \"나 는 학교 에 간 다\" or \"학교\"\n",
    "    # sent = ' '.join(mecab('우박이라고함'))\n",
    "    \n",
    "    encoded_batch = tokenizer.batch_encode_plus(sents, return_tensors=\"pt\",padding=True)\n",
    "    output = get_hidden_states(encoded_batch, model)\n",
    "    output = output[pool_method-1]\n",
    "    \n",
    "    token_ids_word_li = []\n",
    "    for i in range(len(sents)):\n",
    "        idx = get_word_idx(sents[i], words[i])\n",
    "        token_ids_word = np.where(np.array(encoded_batch.word_ids(i)) == idx)\n",
    "        token_ids_word_li.append(token_ids_word)\n",
    "        \n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_outputs = []\n",
    "    for i in range(len(token_ids_word_li)):\n",
    "        word_tokens_output = output[i, token_ids_word_li[i][0],:].mean(dim=0).squeeze()\n",
    "        word_tokens_outputs.append(word_tokens_output)\n",
    "    word_tokens_outputs = torch.stack(word_tokens_outputs)\n",
    "    return word_tokens_outputs\n",
    "\n",
    "def get_word_vector_batch(words, tokenizer, model, pool_method:int):\n",
    "    # sent : tokenized and joined with space; ex. \"나 는 학교 에 간 다\" or \"학교\"\n",
    "    # sent = ' '.join(mecab('우박이라고함'))\n",
    "    \n",
    "    encoded_batch = tokenizer.batch_encode_plus(words, return_tensors=\"pt\", padding=True)\n",
    "    output = get_hidden_states(encoded_batch, model)\n",
    "\n",
    "\n",
    "    output = output[pool_method-1]\n",
    "    token_ids_word = (np.array([1]),) # [CLS] token\n",
    "\n",
    "    # Only select the tokens that constitute the requested word\n",
    "    word_tokens_output = output[:,token_ids_word,:].squeeze()\n",
    "    return word_tokens_output\n",
    "\n",
    "# layers = [-4, -3, -2, -1]\n",
    "\n",
    "# sent = \"장갑을 낀 사람이 설원에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다\"\n",
    "# word = '설원'\n",
    "\n",
    "# for answer in [\"밭\", \"바다\", \"갯벌\", \"설원\", \"계곡\"]:\n",
    "#     idx = get_word_idx(sent, answer)\n",
    "#     word_embedding = get_orgword_vector(sent, idx, tokenizer, model, layers, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = '장갑을 낀 사람이 밭에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다'\n",
    "b = '장갑'\n",
    "\n",
    "tokenizer(\n",
    "                b,\n",
    "                max_length = tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation = False,\n",
    "                add_special_tokens = True,\n",
    "                padding = 'max_length'\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last hidden state shape: torch.Size([1, 512, 768])\n",
      "hidden state shape: torch.Size([13, 1, 512, 768])\n",
      "attentions shape: torch.Size([12, 1, 12, 512, 512])\n",
      "last hidden state shape: torch.Size([1, 512, 768])\n",
      "hidden state shape: torch.Size([13, 1, 512, 768])\n",
      "attentions shape: torch.Size([12, 1, 12, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def todevice(device, tokenized):\n",
    "    return {k:v.to(device) for k,v in tokenized.items()}\n",
    "\n",
    "a = '장갑을 낀 사람이 밭에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다'\n",
    "b = '장갑'\n",
    "\n",
    "def get_word_emb(sent):\n",
    "    tokenized = tokenizer(\n",
    "                sent,\n",
    "                max_length = tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\",\n",
    "                truncation = False,\n",
    "                add_special_tokens = True,\n",
    "                padding = 'max_length'\n",
    "                )\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    last_hidden_state, hidden_states, attentions = model(**todevice(device, tokenized),\n",
    "                                                                    output_hidden_states = True, \n",
    "                                                                    output_attentions=True,\n",
    "                                                                    return_dict=False) # return_dict=True 시 outputs = model(**pt)로 return 받아야\n",
    "    # 512 words\n",
    "    print('last hidden state shape:', last_hidden_state.shape)\n",
    "\n",
    "    # 13:(initial embeddings + 12 BERT layers)\n",
    "    print('hidden state shape:', torch.stack(hidden_states).size()) \n",
    "\n",
    "    print('attentions shape:', torch.stack(attentions).size())\n",
    "\n",
    "    # Remove batch-dim, Swap dimensions 0 and 1.\n",
    "    token_embeddings = torch.stack(hidden_states).squeeze().permute(1,0,2)\n",
    "    token_embeddings.size() # [512, 13, 768] (maxlen, layer, dimension)\n",
    "\n",
    "    #https://is-rajapaksha.medium.com/bert-word-embeddings-deep-dive-32f6214f02bf\n",
    "\n",
    "    # initial embeddings can be taken from 0th layer of hidden states\n",
    "    # hidden_states = (vocabs, layers, dimension)\n",
    "\n",
    "    word_embed_2 = token_embeddings[:,0,:] # [512, 13, 768] -> [512, 768]\n",
    "    # sum of all hidden states\n",
    "    word_embed_3 = token_embeddings.sum(1) # [512, 13, 768] -> [512, 768]\n",
    "    # sum of second to last layer\n",
    "    word_embed_4 = token_embeddings[:,2:,:].sum(1) # [512, 13, 768] -> [512, 11, 768] -> [512, 768]\n",
    "    # sum of last four layer\n",
    "    word_embed_5 = token_embeddings[:,-4:,:].sum(1) # [512, 4, 768] -> [512, 768]\n",
    "    # concatenate last four layers\n",
    "    word_embed_6 = torch.cat([token_embeddings[:,i,:] for i in range(-4, -1+1)], dim=1) # [512, 13, 768] -> [512, 4, 768] -> [512, 4*768]\n",
    "    return word_embed_2, word_embed_3, word_embed_4, word_embed_5, word_embed_6\n",
    "\n",
    "token_embeddings_a = get_word_emb(a)[0]\n",
    "token_embeddings_b = get_word_emb(b)[0]\n",
    "\n",
    "#token_embeddings_a[0]==token_embeddings_b[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, 768]) torch.Size([512, 768]) torch.Size([512, 768]) torch.Size([512, 768]) torch.Size([512, 3072])\n"
     ]
    }
   ],
   "source": [
    "# Remove batch-dim, Swap dimensions 0 and 1.\n",
    "token_embeddings = torch.stack(hidden_states).squeeze().permute(1,0,2)\n",
    "token_embeddings.size() # [512, 13, 768] (maxlen, layer, dimension)\n",
    "\n",
    "#https://is-rajapaksha.medium.com/bert-word-embeddings-deep-dive-32f6214f02bf\n",
    "\n",
    "# initial embeddings can be taken from 0th layer of hidden states\n",
    "# hidden_states = (vocabs, layers, dimension)\n",
    "\n",
    "word_embed_2 = token_embeddings[:,0,:] # [512, 13, 768] -> [512, 768]\n",
    "# sum of all hidden states\n",
    "word_embed_3 = token_embeddings.sum(1) # [512, 13, 768] -> [512, 768]\n",
    "# sum of second to last layer\n",
    "word_embed_4 = token_embeddings[:,2:,:].sum(1) # [512, 13, 768] -> [512, 11, 768] -> [512, 768]\n",
    "# sum of last four layer\n",
    "word_embed_5 = token_embeddings[:,-4:,:].sum(1) # [512, 4, 768] -> [512, 768]\n",
    "# concatenate last four layers\n",
    "word_embed_6 = torch.cat([token_embeddings[:,i,:] for i in range(-4, -1+1)], dim=1) # [512, 13, 768] -> [512, 4, 768] -> [512, 4*768]\n",
    "print(\n",
    "word_embed_2.size(),\n",
    "word_embed_3.size(),\n",
    "word_embed_4.size(),\n",
    "word_embed_5.size(),\n",
    "word_embed_6.size(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate the average of all 512 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['장갑',\n",
       " '##을',\n",
       " '낀',\n",
       " '사람',\n",
       " '##이',\n",
       " '밭',\n",
       " '##에',\n",
       " '있',\n",
       " '##다',\n",
       " '그',\n",
       " '사람',\n",
       " '##이',\n",
       " '허리',\n",
       " '##를',\n",
       " '숙이',\n",
       " '##고',\n",
       " '있',\n",
       " '##다',\n",
       " '그',\n",
       " '사람',\n",
       " '##이',\n",
       " '작업',\n",
       " '##을',\n",
       " '한다']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenize(\"장갑을 낀 사람이 밭에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `hidden_states` has shape [13 x 1 x 22 x 768]\n",
    "# `token_vecs` is a tensor with shape [22 x 768]\n",
    "token_vecs = hidden_states[-2][0]\n",
    "\n",
    "# Calculate the average of all 22 token vectors.\n",
    "sentence_embedding = torch.mean(token_vecs, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "def get_kobert_sentvector(sent_A, \n",
    "                          to_get_cls:bool = False, \n",
    "                          start_idx_layer:int = None, \n",
    "                          end_idx_layer:int = None,\n",
    "                          print_outputs_shapes = False):\n",
    "\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    \n",
    "    tokens_pt = tokenizer(\n",
    "            sent_A,\n",
    "            max_length = tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation = False,\n",
    "            add_special_tokens = True,\n",
    "            padding = 'max_length'\n",
    "    )\n",
    "\n",
    "    tokens_pt = {k: v.to(device) for k, v in tokens_pt.items()}\n",
    "\n",
    "    with torch.no_grad():# 그라디엔트 계산 비활성화\n",
    "        last_hidden_state, pooler_output, hidden_states, attentions = model(**tokens_pt, \n",
    "                                                                            output_hidden_states = True, \n",
    "                                                                            output_attentions=True,\n",
    "                                                                            return_dict=False) # return_dict=True 시 outputs = model(**pt)로 return 받아야\n",
    "    hidden_states, attentions = map(torch.cat, [hidden_states, attentions])\n",
    "\n",
    "    if print_outputs_shapes:\n",
    "        print('last_hidden_state.shape', last_hidden_state.shape)\n",
    "        print('pooler_output.shape', pooler_output.shape)\n",
    "        print('hidden_states.shape', hidden_states.shape)\n",
    "        print('attentions.shape', attentions.shape)\n",
    "\n",
    "    if to_get_cls:\n",
    "        logits = last_hidden_state[:,0,:].detach().cpu().numpy()\n",
    "    \n",
    "    if not to_get_cls:\n",
    "        assert isinstance(start_idx_layer, int) and isinstance(end_idx_layer, int), \"set start and end layer index arguments\"\n",
    "        # hidden_states:tuple\n",
    "        #logits = torch.cat(hidden_states)[:, start_idx_layer:end_idx_layer+1, :].detach().cpu().numpy()\n",
    "        logits = hidden_states[:, start_idx_layer:end_idx_layer, :].detach().cpu().numpy()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return logits\n",
    "\n",
    "# sep = chr(255)\n",
    "# temp_li = train_article_data['text'].tolist()\n",
    "# temp_doc = temp_li[0]\n",
    "sent_A = '장갑을 낀 사람이 밭에 있다\\n그 사람이 허리를 숙이고 있다\\n그 사람이 작업을 한다'\n",
    "\n",
    "layer_num = 13 # kobert layer num (+emb layer)\n",
    "\n",
    "logits = get_kobert_sentvector(sent_A, to_get_cls = True)\n",
    "print(logits.shape)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1c1fe2c09c8b815ea75f54ef7360951519120caf190d7dbe146b520842d17cb5"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('vqa12100': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
